{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this chapter we created own deep learning framework  \n",
    "каждый тензор получает два новых атрибута. \n",
    "\n",
    "**creators** — это список любых тензоров, использовавшихся для создания текущего тензора (по умолчанию имеет значение None).То есть если тензор z получается сложением двух других тензоров, x и y, атрибут creators тензора z будет содержать тензоры x и y\n",
    "\n",
    "**creation_op** - опутствующий атрибут, который хранит операции, использовавшиеся в процессе создания данного тензора. *То есть инструкция z = x + y создаст вычислительный граф с тремя узлами (x, y и z) и двумя ребрами (z -> x и z -> y). Каждое ребро при этом подписано операцией из creation_op, то есть add.* Этот граф поможет организовать рекурсивное обратное распространение градиентов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, data, creators=None, creation_op=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "    \n",
    "    # Автоматическое вычисление градиента\n",
    "    def backward(self, grad):\n",
    "        self.grad = grad\n",
    "        if (self.creation_op == 'add'):\n",
    "            self.creators[0].backward(grad)\n",
    "            self.creators[1].backward(grad)\n",
    "        \n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return Tensor(self.data + other.data,\n",
    "                        creators=[self, other],\n",
    "                        creation_op='add')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor — возможность использовать граф для вычисления градиентов. **Если вызвать метод z.backward()**, он передаст градиент для x и y с учетом функции, с помощью которой создавался тензор z (add). Как показано в примере выше, мы передаем вектор градиентов (np.array([1,1,1,1,1])) в z, а тот применяет его к своим родителям. Самой замечательной особенностью этой формы автоматического вычисления градиента является то, что она работает рекурсивно — каждый вектор вызывает метод .backward() всех своих родителей из списка self.creator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[array([1, 2, 3, 4, 5]), array([2, 2, 2, 2, 2])]\n",
      "add\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([1, 2, 3, 4, 5])\n",
    "y = Tensor([2, 2, 2, 2, 2])\n",
    "z = x + y\n",
    "z.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
    "\n",
    "# В данном случае у нас только один градиент для добавления в x and y\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.creators)\n",
    "print(z.creation_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5])\n",
    "b = Tensor([2,2,2,2,2])\n",
    "c = Tensor([5,4,3,2,1])\n",
    "\n",
    "d = a + b\n",
    "e = b + c\n",
    "f = d + e\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "\n",
    "b.grad.data == np.array([2,2,2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Много кратное использование тензоров\n",
    "Выше реализация имеет ошибку, нам нужно, что в случае многократного использования тензоров,  при накапливание градиента она получала градиент от всех потомков.\n",
    "\n",
    "* Дополнительно **мы создали счетчик self.children, подсчитывающий количество градиентов, полученных от каждого потомка в процессе обратного распространения.** Так мы исключаем возможность случайного повторного обратного распространения от одного и того же потомка (в этом случае возбуждается исключение).\n",
    "*  **all_children_grads_accounted_for(). Эта функция проверяет, были ли получены градиенты от всех потомков в графе.**\n",
    "* Обычно, когда метод .backward() вызывается промежуточной переменной в графе, она тут же вызывает метод .backward() своего родителя. Но так как некоторые переменные получают свои градиенты от нескольких родителей, каждая должна отложить вызов .backward() своего родителя, пока не получит свой окончательный градиент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children): # Скорректировать число потомков данного тезнора\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "    \n",
    "    # Проверка на получили ли тензоры градиенты от всех потомков\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id, cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True        \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    "            if(grad is None):\n",
    "                grad = FloatTensor(np.ones_like(self.data))\n",
    "            # Проверка возможности обратного распространения или ожидания градиента\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad # Накопление градиента от нескольких потомков\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    # Фактически начало обратного распространения\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        # Включаем автограф\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5], autograd=True)\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\n",
    "c = Tensor([5,4,3,2,1], autograd=True)\n",
    "\n",
    "d = a + b\n",
    "e = b + c\n",
    "f = d + e\n",
    "\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "\n",
    "print(b.grad.data == np.array([2,2,2,2,2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сложение в обратном распространении\n",
    "А так выполняется обратное распространение градиента через функцию сложения в методе .backward():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if(self.creation_op == \"add\"):\n",
    "#   self.creators[0].backward(self.grad, self)\n",
    "#   self.creators[1].backward(self.grad, self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что сложение нигде в классе не обрабатывается. Логика обратного распространения абстрагирована, поэтому все необходимое для поддержки сложения определяется в этих двух местах. **Также обратите внимание, что логика обратного распространения вызывает .backward() дважды, по одному разу для каждой переменной, участвовавшей в сложении. То есть логика по умолчанию обратного распространения заключается в обратном распространении через каждую переменную в графе.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Добавление отрицания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children): # Скорректировать число потомков данного тезнора\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "    \n",
    "    # Проверка на получили ли тензоры градиенты от всех потомков\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id, cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True        \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    "            if(grad is None):\n",
    "                grad = FloatTensor(np.ones_like(self.data))\n",
    "            # Проверка возможности обратного распространения или ожидания градиента\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad # Накопление градиента от нескольких потомков\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    # Фактически начало обратного распространения\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    # Поскольку в операции отрицания __neg__ участвует только один родитель, \n",
    "                    # метод .backward() должен вызываться только один раз.\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "    \n",
    "    # Сложение двух тензоров           \n",
    "    def __add__(self, other):\n",
    "        # Включаем автограф\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    # Операция меняет знак на минус \n",
    "    def __neg__(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(-self.data,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"neg\")\n",
    "        return Tensor(-self.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5], autograd=True)\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\n",
    "c = Tensor([5,4,3,2,1], autograd=True)\n",
    "\n",
    "d = a + (-b)\n",
    "e = (-b) + c\n",
    "f = d + e\n",
    "\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "\n",
    "print(b.grad.data == np.array([-2,-2,-2,-2,-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавление других ариф операций "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children): # Скорректировать число потомков данного тезнора\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "    \n",
    "    # Проверка на получили ли тензоры градиенты от всех потомков\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id, cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True        \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    "            if(grad is None):\n",
    "                grad = FloatTensor(np.ones_like(self.data))\n",
    "            # Проверка возможности обратного распространения или ожидания градиента\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad # Накопление градиента от нескольких потомков\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    # Фактически начало обратного распространения\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    # Поскольку в операции отрицания __neg__ участвует только один родитель, \n",
    "                    # метод .backward() должен вызываться только один раз.\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(self.grad)\n",
    "                    \n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                         \n",
    "    # Сложение двух тензоров           \n",
    "    def __add__(self, other):\n",
    "        # Включаем автограф\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    # Операция меняет знак на минус \n",
    "    def __neg__(self):\n",
    "        if (self.autograd):\n",
    "            return Tensor(-self.data,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"neg\")\n",
    "        return Tensor(-self.data)\n",
    "    \n",
    "    # Вычитание\n",
    "    def __sub__(self, other):\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                         autograd=True,\n",
    "                         creators=[self, other],\n",
    "                         creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    # Умножение \n",
    "    def __mul__(self, other):\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                         autograd=True,\n",
    "                         creators=[self, other],\n",
    "                         creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    # Функция sum выполняет сложение элементов тензора по измерениям\n",
    "    def sum(self, dim):\n",
    "        if (self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"sum_\" + str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    # Функция expand используется для обратного распространения операции .sum(). \n",
    "    # Она копирует данные по измерению.\n",
    "    def expand(self, dim, copies):\n",
    "        \n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "\n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data)) \n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конце сети вычисляются градиенты, после чего сигнал ошибки начинает распространяться через сеть в обратном направлении вызовом функций, соответствующих функциям, которые использовались для прямого распространения активаций. **Если последней была операция умножения матриц (а это так и есть), в обратном распространении производится умножение матрицы (dot) на транспонированную матрицу.**\n",
    "\n",
    "На следующем рисунке это происходит **в строке layer_1_delta=layer_2_delta.dot (weights_1_2.T). В предыдущем коде это происходит в инструкции if(self.creation_op == \"mm\")** (соответствующие строки выделены жирным). Здесь мы выполняем те же операции, что и прежде (в порядке, обратном порядку прямого распространения), но код организован гораздо лучше.\n",
    "\n",
    "Отрывок из книги: Траск Э. «Грокаем глубокое обучение». Apple Books. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Записи о SUM and Expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor(np.array([[1,2,3],\n",
    "                     [4,5,6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number it's demension of summary \n",
    "x.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 15])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3]],\n",
       "\n",
       "       [[4, 4, 4, 4, 4],\n",
       "        [5, 5, 5, 5, 5],\n",
       "        [6, 6, 6, 6, 6]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.expand(dim=2, copies=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Используем наш фреймворк и автограф для тренировки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58128304]\n",
      "[0.42945783]\n",
      "[0.31080708]\n",
      "[0.22343058]\n",
      "[0.1777608]\n",
      "[0.16803502]\n",
      "[0.25763598]\n",
      "[0.32192563]\n",
      "[0.61786922]\n",
      "[0.37022632]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "w = list()\n",
    "w.append(Tensor(np.random.rand(2,3), autograd=True))\n",
    "w.append(Tensor(np.random.rand(3,1), autograd=True))\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    # Predict\n",
    "    pred = data.mm(w[0]).mm(w[1])\n",
    "    \n",
    "    # Compare\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    \n",
    "    # Learn\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "\n",
    "    for w_ in w:\n",
    "        w_.data -= w_.grad.data * 0.1\n",
    "        w_.grad.data *= 0\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

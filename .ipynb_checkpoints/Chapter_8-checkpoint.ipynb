{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Topics\n",
    "* Regularization\n",
    "* Overfitting\n",
    "* Dropout\n",
    "* Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary network \n",
    "В этой главе мы познакомимся с основами регуляризации — ключевым средством в борьбе против переобучения нейронных сетей. С этой целью мы сразу возьмем самую мощную нейронную сеть из имеющихся у нас (трехслойную сеть со скрытым слоем relu) и используем ее для решения задачи классификации рукописных цифр из набора данных MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(data, labels, num_epochs:int, weight:list, j:int, test_data=False, alpha=0.001, printing=50):\n",
    "    \"\"\"\n",
    "        Fitting Neural Network\n",
    "        1) Make a layers\n",
    "        2) Counting Error \n",
    "        3) For Train Data: Update Weights\n",
    "    \"\"\"\n",
    "    error, correct_answer = (0, 0.0)\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        layer_0 = data[i:i+1]                      # (1, 784)\n",
    "        layer_1 = relu(np.dot(layer_0, weight[0])) # (1, 40) =  (1, 784) * (784, 40)\n",
    "        layer_2 = np.dot(layer_1, weight[1])       # (1, 10) =  (1, 40) * (40, 10)\n",
    "        \n",
    "        # Labels [0, 0, 0, 1 ... 0] - Layers_2 [-0.1, -0.5, 0.06 ... 0.2]\n",
    "        # Если будем без np.sum, то будет массив из 10 элементов, а так мы суммируем весь лист\n",
    "        error += np.sum((labels[i:i+1] - layer_2) ** 2)\n",
    "        # Argmax Return Position Max Value In List\n",
    "        # Когда позиции макс значений совпадает тогда 1\n",
    "        correct_answer += np.argmax(layer_2) == np.argmax(labels[i:i+1])\n",
    "        \n",
    "        # Нам не нужно пересчитывать веса для теста\n",
    "        if test_data==False:\n",
    "            \n",
    "            layer_2_delta = labels[i:i+1] - layer_2\n",
    "            layer_1_delta = np.dot(layer_2_delta, weight[1].T) * relu2deriv(layer_1)\n",
    "\n",
    "            weight[1] += alpha * layer_1.T.dot(layer_2_delta)\n",
    "            weight[0] += alpha * np.dot(layer_0.T, layer_1_delta)\n",
    "        \n",
    "    \n",
    "    \n",
    "    if test_data == False:\n",
    "        # Вывод каждые 50 или самая последняя\n",
    "        if(j % printing == 0 or j == num_epochs - 1):\n",
    "            # [0:5] выводим первые 5 чисел от ошибки\n",
    "            # Чтобы понять какой процент ошибки делем на общее число примеров 1000\n",
    "            print(\"\\nEpochs:\" + str(j) +\n",
    "              \"\\nTrain-Err:\" + str(error / float(len(data)))[0:5] +\n",
    "              \" Train-Acc:\" + str(correct_answer / float(len(data))))\n",
    "    \n",
    "    else: # Для тестовых данных\n",
    "        if(j % printing == 0 or j == num_epochs - 1):\n",
    "            print(\"Test-Err:\" + str(error / float(len(data)))[0:5] +\n",
    "              \"  Test-Acc:\" + str(correct_answer / float(len(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs:0\n",
      "Train-Err:0.952 Train-Acc:0.215\n",
      "Test-Err:0.850  Test-Acc:0.337\n",
      "\n",
      "Epochs:50\n",
      "Train-Err:0.192 Train-Acc:0.946\n",
      "Test-Err:0.354  Test-Acc:0.839\n",
      "\n",
      "Epochs:100\n",
      "Train-Err:0.118 Train-Acc:0.977\n",
      "Test-Err:0.325  Test-Acc:0.841\n",
      "\n",
      "Epochs:150\n",
      "Train-Err:0.085 Train-Acc:0.991\n",
      "Test-Err:0.323  Test-Acc:0.844\n",
      "\n",
      "Epochs:200\n",
      "Train-Err:0.065 Train-Acc:0.995\n",
      "Test-Err:0.326  Test-Acc:0.842\n",
      "\n",
      "Epochs:250\n",
      "Train-Err:0.052 Train-Acc:0.996\n",
      "Test-Err:0.329  Test-Acc:0.838\n",
      "\n",
      "Epochs:300\n",
      "Train-Err:0.042 Train-Acc:0.997\n",
      "Test-Err:0.333  Test-Acc:0.831\n",
      "\n",
      "Epochs:349\n",
      "Train-Err:0.036 Train-Acc:0.997\n",
      "Test-Err:0.336  Test-Acc:0.83\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Делаем train_images (1000, 784) то есть одно изображение это вектор (784, 1)\n",
    "train_images, train_labels = (x_train[0:1000].reshape(1000, 28*28) / 255, y_train[0:1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(train_labels), 10)) # Заполняем нулями для лабелов (1000, 10)\n",
    "\n",
    "# Enumerate - отдает два числа индекс и номер этого индекса и ставим единицу \n",
    "for index, label in enumerate(train_labels):\n",
    "    one_hot_labels[index][label] = 1\n",
    "\n",
    "# [0, 0, 0, 1 ... 0]\n",
    "train_labels = one_hot_labels\n",
    "\n",
    "#Тоже самое, что и для train\n",
    "test_images = x_test[0:1000].reshape(1000, 28*28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for index, label in enumerate(y_test):\n",
    "    test_labels[index][label] = 1\n",
    "    \n",
    "relu = lambda x: (x > 0) * x\n",
    "relu2deriv = lambda output: (output > 0)\n",
    "\n",
    "alpha, epochs, hidden_layer, pixel_per_image, num_labels = (0.001, 350, 40, 784, 10)\n",
    "\n",
    "weight_0_1 = 0.2 * np.random.random((pixel_per_image, hidden_layer)) - 0.1\n",
    "weight_1_2 = 0.2 * np.random.random((hidden_layer, num_labels)) - 0.1\n",
    "\n",
    "for j in range(epochs):\n",
    "    # Train\n",
    "    training(train_images, train_labels, epochs, [weight_0_1, weight_1_2], j)\n",
    "    # Test\n",
    "    training(test_images, test_labels, epochs, [weight_0_1, weight_1_2], j, test_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logs\n",
    "\n",
    "Epochs:0\n",
    "Train-Err:0.952 Train-Acc:0.215\n",
    "Test-Err:0.850  Test-Acc:0.337\n",
    "\n",
    "Epochs:50\n",
    "Train-Err:0.192 Train-Acc:0.946\n",
    "Test-Err:0.354  Test-Acc:0.839\n",
    "\n",
    "Epochs:100\n",
    "Train-Err:0.118 Train-Acc:0.977\n",
    "Test-Err:0.325  Test-Acc:0.841\n",
    "\n",
    "Epochs:150\n",
    "Train-Err:0.085 Train-Acc:0.991\n",
    "Test-Err:0.323  Test-Acc:0.844\n",
    "\n",
    "Epochs:200\n",
    "Train-Err:0.065 Train-Acc:0.995\n",
    "Test-Err:0.326  Test-Acc:0.842\n",
    "\n",
    "Epochs:250\n",
    "Train-Err:0.052 Train-Acc:0.996\n",
    "Test-Err:0.329  Test-Acc:0.838\n",
    "\n",
    "Epochs:300\n",
    "Train-Err:0.042 Train-Acc:0.997\n",
    "Test-Err:0.333  Test-Acc:0.831\n",
    "\n",
    "Epochs:349\n",
    "Train-Err:0.036 Train-Acc:0.997\n",
    "Test-Err:0.336  Test-Acc:0.83\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Dropout\n",
    "Чтобы сделать дропаут нам нужно сделать маску из нулей и единиц и умножить на основной слой\n",
    "\n",
    "**Важно:** Чтобы слой имеел столько же значения, **мы умножаем на 2**, так как у нас половина слоя стало с нулями, поэтому другую половину, надо увеличить в два раза, для баланса или взвешенной суммы.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout In Code\n",
    "\n",
    "# 1)Делаем макску из 0 и 1, то есть [0, 1, 0, 1]\n",
    "# dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
    "\n",
    "# 2) Умножаем на два \n",
    "# layer_1 *= dropout_mask * 2\n",
    "\n",
    "# 3) Пересчитываем дельту \n",
    "# layer_1_delta *= dropout_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_with_dropout(data, labels, num_epochs:int, weight:list, j:int, test_data=False, alpha=0.001, printing=50):\n",
    "\n",
    "    \n",
    "    error, correct_answer = (0, 0.0)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        layer_0 = data[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0, weight[0])) # (1, 40)\n",
    "        \n",
    "        # !NEW LINE:\n",
    "        # Делаем Dropout маску и заполняем ее 0 и 1 \n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        # Чтобы слой имеел столько же значения мы умножаем на 2,  \n",
    "        # так как у нас половина слоя стало с нулями, поэтому другую половину, \n",
    "        # надо увеличить в два раза, для баланса или взвешенной суммы.\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        \n",
    "        layer_2 = np.dot(layer_1, weight[1])\n",
    "        \n",
    "        \n",
    "        error += np.sum((labels[i:i+1] - layer_2) ** 2)\n",
    "        correct_answer += np.argmax(layer_2) == np.argmax(labels[i:i+1])\n",
    "        \n",
    "      \n",
    "        if test_data==False:\n",
    "            layer_2_delta = labels[i:i+1] - layer_2\n",
    "            layer_1_delta = np.dot(layer_2_delta, weight[1].T) * relu2deriv(layer_1)\n",
    "            \n",
    "            # !NEW LINE:\n",
    "            layer_1_delta *= dropout_mask\n",
    "            \n",
    "            weight[1] += alpha * layer_1.T.dot(layer_2_delta)\n",
    "            weight[0] += alpha * np.dot(layer_0.T, layer_1_delta)\n",
    "        \n",
    "    \n",
    "    \n",
    "    if test_data == False:\n",
    "        if(j % printing == 0 or j == num_epochs - 1):\n",
    "            print(\"\\nEpochs:\" + str(j) +\n",
    "              \"\\nTrain-Err:\" + str(error / float(len(data)))[0:5] +\n",
    "              \" Train-Acc:\" + str(correct_answer / float(len(data))))\n",
    "    else: \n",
    "        if(j % printing == 0 or j == num_epochs - 1):\n",
    "            print(\"Test-Err:\" + str(error / float(len(data)))[0:5] +\n",
    "              \"  Test-Acc:\" + str(correct_answer / float(len(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs:0\n",
      "Train-Err:1.236 Train-Acc:0.17\n",
      "Test-Err:1.029  Test-Acc:0.231\n",
      "\n",
      "Epochs:50\n",
      "Train-Err:0.381 Train-Acc:0.832\n",
      "Test-Err:0.506  Test-Acc:0.71\n",
      "\n",
      "Epochs:100\n",
      "Train-Err:0.312 Train-Acc:0.885\n",
      "Test-Err:0.475  Test-Acc:0.72\n",
      "\n",
      "Epochs:150\n",
      "Train-Err:0.269 Train-Acc:0.927\n",
      "Test-Err:0.446  Test-Acc:0.761\n",
      "\n",
      "Epochs:200\n",
      "Train-Err:0.251 Train-Acc:0.932\n",
      "Test-Err:0.438  Test-Acc:0.767\n",
      "\n",
      "Epochs:250\n",
      "Train-Err:0.231 Train-Acc:0.955\n",
      "Test-Err:0.422  Test-Acc:0.771\n",
      "\n",
      "Epochs:299\n",
      "Train-Err:0.223 Train-Acc:0.96\n",
      "Test-Err:0.420  Test-Acc:0.785\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "train_images, train_labels = (x_train[0:1000].reshape(1000, 28*28) / 255, y_train[0:1000])\n",
    "one_hot_labels = np.zeros((len(train_labels), 10)) \n",
    "for index, label in enumerate(train_labels):\n",
    "    one_hot_labels[index][label] = 1\n",
    "train_labels = one_hot_labels\n",
    "\n",
    "\n",
    "test_images = x_test[0:1000].reshape(1000, 28*28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for index, label in enumerate(y_test):\n",
    "    test_labels[index][label] = 1\n",
    "    \n",
    "relu = lambda x: (x > 0) * x\n",
    "relu2deriv = lambda output: (output > 0)\n",
    "\n",
    "alpha, epochs, hidden_layer, pixel_per_image, num_labels = (0.005, 300, 100, 784, 10)\n",
    "\n",
    "weight_0_1 = 0.2 * np.random.random((pixel_per_image, hidden_layer)) - 0.1\n",
    "weight_1_2 = 0.2 * np.random.random((hidden_layer, num_labels)) - 0.1\n",
    "\n",
    "for j in range(epochs):\n",
    "    \n",
    "    # With Dropout\n",
    "    training_with_dropout(train_images, train_labels, epochs, [weight_0_1, weight_1_2], j)\n",
    "    training_with_dropout(test_images, test_labels, epochs, [weight_0_1, weight_1_2], j, test_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как было показано в примере выше, нейронная сеть **(без прореживания) достигла точности 81.14 % на контрольных данных, после чего, к концу обучения, ее точность упала до 70.73 %**. \n",
    "\n",
    "**После добавления прореживания** сеть ведет себя иначе.\n",
    "**Сеть не только достигает максимума с оценкой 82.36 %, но и прекрасно справляется с эффектом переобучения, завершая обучение с точностью 81.81 % на контрольных данных.** Обратите внимание, что прореживание также замедляет обучение — рост показателя Training-Acc, который в предыдущем примере достиг 100 % и остановился. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Batch Size\n",
    "Этот метод увеличивает скорость обучения и улучшает сходимость. \n",
    "\n",
    "Прежде мы передавали обучающие примеры в сеть по одному, корректируя веса после каждого из них. **Теперь попробуем передавать сети сразу по 100 примеров**, усредняя корректирующие значения для весов по всем 100 примерам. \n",
    "\n",
    "Как оказывается, отдельные обучающие примеры несут в себе очень много шума, в смысле корректирующих значений, которые они генерируют. То есть усреднение делает процесс обучения более плавным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_with_batchsize(data, labels, test_images, test_labels, num_epochs:int, weight:list, j:int, batch_size=100, test_data=False, alpha=0.001, printing=50):\n",
    "    \n",
    "    error, correct_answer = (0, 0.0)\n",
    "    \n",
    "    # NEW LINE:\n",
    "    # Делем общее число на размер батча(range(10))\n",
    "    for i in range(int(len(data) / batch_size)):\n",
    "        # Делаем начальный батч и конечный \n",
    "        batch_start, batch_end = ((i * batch_size), ((i+1)*batch_size))\n",
    "        \n",
    "        layer_0 = data[batch_start:batch_end]\n",
    "        layer_1 = relu(np.dot(layer_0, weight[0])) # (1, 40)\n",
    "        \n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        \n",
    "        layer_2 = np.dot(layer_1, weight[1])\n",
    "        \n",
    "        # Считаем ошибку с батчем \n",
    "        error += np.sum((labels[batch_start:batch_end] - layer_2) ** 2)\n",
    "        # Смотрим каждый экземпляр в батче и считаем правильные ответы, после корректируем веса\n",
    "        for k in range(batch_size):\n",
    "            \n",
    "            correct_answer += int(np.argmax(layer_2[k : k+1]) == \\\n",
    "                                  np.argmax(labels[batch_start + k : batch_start + k + 1]))\n",
    "\n",
    "            layer_2_delta = (labels[batch_start:batch_end] - layer_2) / batch_size\n",
    "            layer_1_delta = layer_2_delta.dot(weight[1].T) * relu2deriv(layer_1)\n",
    "            layer_1_delta *= dropout_mask\n",
    "\n",
    "            weight[1] += alpha * layer_1.T.dot(layer_2_delta)\n",
    "            weight[0] += alpha * layer_0.T.dot(layer_1_delta)\n",
    "            \n",
    "    if(j%10 == 0):\n",
    "        test_error = 0.0\n",
    "        test_correct_cnt = 0\n",
    "\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(np.dot(layer_0,weight[0]))\n",
    "            layer_2 = np.dot(layer_1, weight[1])\n",
    "\n",
    "            test_error += np.sum((test_labels[i:i+1] - layer_2) ** 2)\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "\n",
    "        print(\"\\n\" + \\\n",
    "                \"I:\" + str(j) + \\\n",
    "                \" Test-Err:\" + str(test_error/ float(len(test_images)))[0:5] +\\\n",
    "                \" Test-Acc:\" + str(test_correct_cnt/ float(len(test_images)))+\\\n",
    "                \" Train-Err:\" + str(error/ float(len(data)))[0:5] +\\\n",
    "                \" Train-Acc:\" + str(correct_answer / float(len(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Err:0.839 Test-Acc:0.35 Train-Err:1.286 Train-Acc:0.151\n",
      "\n",
      "I:10 Test-Err:0.572 Test-Acc:0.691 Train-Err:0.583 Train-Acc:0.66\n",
      "\n",
      "I:20 Test-Err:0.496 Test-Acc:0.734 Train-Err:0.496 Train-Acc:0.748\n",
      "\n",
      "I:30 Test-Err:0.452 Test-Acc:0.769 Train-Err:0.444 Train-Acc:0.792\n",
      "\n",
      "I:40 Test-Err:0.425 Test-Acc:0.791 Train-Err:0.412 Train-Acc:0.806\n",
      "\n",
      "I:50 Test-Err:0.402 Test-Acc:0.801 Train-Err:0.388 Train-Acc:0.833\n",
      "\n",
      "I:60 Test-Err:0.388 Test-Acc:0.817 Train-Err:0.364 Train-Acc:0.843\n",
      "\n",
      "I:70 Test-Err:0.377 Test-Acc:0.824 Train-Err:0.345 Train-Acc:0.865\n",
      "\n",
      "I:80 Test-Err:0.368 Test-Acc:0.821 Train-Err:0.333 Train-Acc:0.859\n",
      "\n",
      "I:90 Test-Err:0.360 Test-Acc:0.825 Train-Err:0.319 Train-Acc:0.883\n",
      "\n",
      "I:100 Test-Err:0.353 Test-Acc:0.826 Train-Err:0.305 Train-Acc:0.89\n",
      "\n",
      "I:110 Test-Err:0.348 Test-Acc:0.831 Train-Err:0.297 Train-Acc:0.893\n",
      "\n",
      "I:120 Test-Err:0.343 Test-Acc:0.825 Train-Err:0.289 Train-Acc:0.899\n",
      "\n",
      "I:130 Test-Err:0.338 Test-Acc:0.83 Train-Err:0.273 Train-Acc:0.902\n",
      "\n",
      "I:140 Test-Err:0.336 Test-Acc:0.834 Train-Err:0.284 Train-Acc:0.902\n",
      "\n",
      "I:150 Test-Err:0.332 Test-Acc:0.833 Train-Err:0.264 Train-Acc:0.921\n",
      "\n",
      "I:160 Test-Err:0.332 Test-Acc:0.836 Train-Err:0.285 Train-Acc:0.914\n",
      "\n",
      "I:170 Test-Err:0.330 Test-Acc:0.835 Train-Err:0.266 Train-Acc:0.926\n",
      "\n",
      "I:180 Test-Err:0.328 Test-Acc:0.836 Train-Err:0.264 Train-Acc:0.925\n",
      "\n",
      "I:190 Test-Err:0.325 Test-Acc:0.842 Train-Err:0.252 Train-Acc:0.93\n",
      "\n",
      "I:200 Test-Err:0.325 Test-Acc:0.837 Train-Err:0.253 Train-Acc:0.932\n",
      "\n",
      "I:210 Test-Err:0.322 Test-Acc:0.845 Train-Err:0.260 Train-Acc:0.932\n",
      "\n",
      "I:220 Test-Err:0.321 Test-Acc:0.842 Train-Err:0.244 Train-Acc:0.936\n",
      "\n",
      "I:230 Test-Err:0.321 Test-Acc:0.846 Train-Err:0.233 Train-Acc:0.946\n",
      "\n",
      "I:240 Test-Err:0.320 Test-Acc:0.843 Train-Err:0.241 Train-Acc:0.937\n",
      "\n",
      "I:250 Test-Err:0.319 Test-Acc:0.845 Train-Err:0.230 Train-Acc:0.948\n",
      "\n",
      "I:260 Test-Err:0.318 Test-Acc:0.842 Train-Err:0.237 Train-Acc:0.943\n",
      "\n",
      "I:270 Test-Err:0.318 Test-Acc:0.847 Train-Err:0.225 Train-Acc:0.947\n",
      "\n",
      "I:280 Test-Err:0.315 Test-Acc:0.851 Train-Err:0.228 Train-Acc:0.945\n",
      "\n",
      "I:290 Test-Err:0.316 Test-Acc:0.842 Train-Err:0.224 Train-Acc:0.953\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "train_images, train_labels = (x_train[0:1000].reshape(1000, 28*28) / 255, y_train[0:1000])\n",
    "one_hot_labels = np.zeros((len(train_labels), 10)) \n",
    "for index, label in enumerate(train_labels):\n",
    "    one_hot_labels[index][label] = 1\n",
    "train_labels = one_hot_labels\n",
    "\n",
    "\n",
    "test_images = x_test[0:1000].reshape(1000, 28*28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for index, label in enumerate(y_test):\n",
    "    test_labels[index][label] = 1\n",
    "    \n",
    "relu = lambda x: (x > 0) * x\n",
    "relu2deriv = lambda output: (output > 0)\n",
    "\n",
    "alpha, epochs, hidden_layer, pixel_per_image, num_labels = (0.005, 300, 100, 784, 10)\n",
    "\n",
    "weight_0_1 = 0.2 * np.random.random((pixel_per_image, hidden_layer)) - 0.1\n",
    "weight_1_2 = 0.2 * np.random.random((hidden_layer, num_labels)) - 0.1\n",
    "\n",
    "for j in range(epochs):\n",
    "    \n",
    "    # With Dropout\n",
    "    training_with_batchsize(train_images, train_labels, test_images, test_labels, epochs, [weight_0_1, weight_1_2], j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
